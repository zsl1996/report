\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}deep learning理论}{1}}
\@writefile{toc}{\contentsline {paragraph}{并非浅层网络表达能力差}{1}}
\@writefile{toc}{\contentsline {paragraph}{深层网络的好处}{1}}
\@writefile{toc}{\contentsline {paragraph}{lowlayer 的参数是最重要的}{1}}
\@writefile{toc}{\contentsline {paragraph}{通过NN的到底是如何拟合fuction的？}{2}}
\@writefile{toc}{\contentsline {paragraph}{optimization deep learning}{2}}
\@writefile{toc}{\contentsline {paragraph}{影响最后收敛的结果的因素}{2}}
\@writefile{toc}{\contentsline {paragraph}{Generation Gap}{2}}
\@writefile{toc}{\contentsline {paragraph}{泛化能力与sensitivity}{3}}
\@writefile{toc}{\contentsline {paragraph}{泛化能力与sharpness}{3}}
\@writefile{toc}{\contentsline {paragraph}{Tips for training DNN}{3}}
\@writefile{toc}{\contentsline {paragraph}{避免overfitting 的方法}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}机器学习理论}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Paper Reading}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}An overview of gradient descent optimization algorithms}{4}}
\@writefile{toc}{\contentsline {paragraph}{Batch gradient descent}{4}}
\@writefile{toc}{\contentsline {paragraph}{Stochastic gradient descent(SGD)}{4}}
\@writefile{toc}{\contentsline {paragraph}{Mini-batch gradient descent}{4}}
\@writefile{toc}{\contentsline {paragraph}{Momentum}{4}}
\@writefile{toc}{\contentsline {paragraph}{Nesterov accelerated gradient(NAG)}{4}}
\@writefile{toc}{\contentsline {paragraph}{Adagrad}{4}}
