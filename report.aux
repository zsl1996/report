\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}deep learning理论}{1}{section.1}}
\@writefile{toc}{\contentsline {paragraph}{并非浅层网络表达能力差}{1}{section*.1}}
\@writefile{toc}{\contentsline {paragraph}{深层网络的好处}{1}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{lowlayer 的参数是最重要的}{1}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{通过NN的到底是如何拟合fuction的？}{2}{section*.4}}
\@writefile{toc}{\contentsline {paragraph}{optimization deep learning}{2}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{影响最后收敛的结果的因素}{2}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{Generation Gap}{2}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{泛化能力与sensitivity}{3}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{泛化能力与sharpness}{3}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Tips for training DNN}{3}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{避免overfitting 的方法}{3}{section*.11}}
\@writefile{toc}{\contentsline {section}{\numberline {2}机器学习理论}{4}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Paper Reading}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}An overview of gradient descent optimization algorithms}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {paragraph}{Batch gradient descent}{4}{section*.12}}
\@writefile{toc}{\contentsline {paragraph}{Stochastic gradient descent(SGD)}{4}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{Mini-batch gradient descent}{4}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{Momentum}{4}{section*.15}}
\@writefile{toc}{\contentsline {paragraph}{Nesterov accelerated gradient(NAG)}{4}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{Adagrad}{4}{section*.17}}
\@writefile{toc}{\contentsline {paragraph}{Adadelta}{5}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{RMSprop}{5}{section*.19}}
\@writefile{toc}{\contentsline {paragraph}{Adam}{5}{section*.20}}
\@writefile{toc}{\contentsline {paragraph}{Adamax}{5}{section*.21}}
\@writefile{toc}{\contentsline {paragraph}{NAdam}{5}{section*.22}}
\@writefile{toc}{\contentsline {paragraph}{which to use}{5}{section*.23}}
\@writefile{toc}{\contentsline {paragraph}{Additional strategies for optimizing SGD}{5}{section*.24}}
